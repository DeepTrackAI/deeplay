{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mappings as Inputs\n",
    "\n",
    "`DeeplayModule`objects support mapping objects (container objects that support arbitrary key lookups) as input, allowing users to define complex data processing pipelines in a simple and intuitive way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "To use mappings as input to `DeeplayModule`objects, the user must define the mapping between the input and output data structures. This is done by configuring the `.set_input_map()` and `.set_output_map()` methods of the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this code defines a linear module that takes a dictionary with a key `x` as input and overwrites the key `x` with the output of the linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[Linear](in_features=10, out_features=64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "import torch\n",
    "\n",
    "module = dl.Layer(torch.nn.Linear, 10, 64)\n",
    "\n",
    "module.set_input_map(\"x\")\n",
    "module.set_output_map(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently supported mapping objects include dictionaries, such as ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add dictionary example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and Torch Geometric Data objects, such as ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "input_data = Data(x=torch.randn(100, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the module is built, the input data can be passed to the module as with `Tensor` objects. The module will automatically map the input and output data to the expected data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  <bound method BaseData.keys of Data(x=[100, 64])>\n",
      "input x shape:  torch.Size([100, 10])\n",
      "output x shape:  torch.Size([100, 64])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "output_data = built(input_data)\n",
    "\n",
    "print(\"output keys: \", output_data.keys)\n",
    "print(\"input x shape: \", input_data.x.shape)\n",
    "print(\"output x shape: \", output_data.x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations with Mappings\n",
    "\n",
    "The `.set_input_map()` method supports local key re-assigment, useful when the input data key doesn't match the expected key by the module. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[MultiheadAttention](embed_dim=10, num_heads=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = dl.Layer(torch.nn.MultiheadAttention, embed_dim=10, num_heads=2)\n",
    "\n",
    "module.set_input_map(query=\"x\", key=\"x\", value=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the `.set_output_map()` method receives a second argument to specify the mapping for the attention weights output ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[MultiheadAttention](embed_dim=10, num_heads=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.set_output_map(\"x\", \"attention_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or alternatively ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[MultiheadAttention](embed_dim=10, num_heads=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.set_output_map(x=0, attention_weights=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where, `x=0` and `attention_weights=1` indicate that `x` and `attention_weights` in the output dictionary correspond to the first and second outputs of the module, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  <bound method BaseData.keys of Data(x=[100, 10], attention_weights=[100, 100])>\n",
      "output x shape:  torch.Size([100, 10])\n",
      "output attention_weights shape:  torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "input_data = Data(x=torch.randn(100, 10))\n",
    "\n",
    "output_data = built(input_data)\n",
    "\n",
    "print(\"output keys: \", output_data.keys)\n",
    "print(\"output x shape: \", input_data.x.shape)\n",
    "print(\"output attention_weights shape: \", output_data.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New keys can be added to the output data structure to store intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[MultiheadAttention](embed_dim=10, num_heads=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = dl.Layer(torch.nn.MultiheadAttention, embed_dim=10, num_heads=2)\n",
    "\n",
    "module.set_input_map(query=\"x\", key=\"x\", value=\"x\")\n",
    "\n",
    "module.set_output_map(\n",
    "    x=0,\n",
    "    intermediate_x=0,\n",
    "    attention_weights=1,\n",
    "    intermediate_attention_weights=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  <bound method BaseData.keys of Data(x=[100, 10], intermediate_x=[100, 10], attention_weights=[100, 100], intermediate_attention_weights=[100, 100])>\n",
      "output x shape:  torch.Size([100, 10])\n",
      "output intermediate_x shape:  torch.Size([100, 10])\n",
      "output attention_weights shape:  torch.Size([100, 100])\n",
      "output intermediate_attention_weights shape:  torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "input_data = Data(x=torch.randn(100, 10))\n",
    "\n",
    "output_data = built(input_data)\n",
    "\n",
    "print(\"output keys: \", output_data.keys)\n",
    "\n",
    "print(\"output x shape: \", input_data.x.shape)\n",
    "print(\"output intermediate_x shape: \", output_data.intermediate_x.shape)\n",
    "\n",
    "print(\"output attention_weights shape: \", output_data.attention_weights.shape)\n",
    "print(\n",
    "    \"output intermediate_attention_weights shape: \",\n",
    "    output_data.intermediate_attention_weights.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mappings Allow Sequential Processing\n",
    "\n",
    "Mapping enables the sequencing of branched pipelines, fully exploiting the modular design of DeeplayModules. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import LayerNorm\n",
    "\n",
    "module = dl.Sequential(\n",
    "    dl.Layer(torch.nn.Bilinear, 10, 10, 10).set_input_map(\"x1\", \"x2\") \\\n",
    "        .set_output_map(\"y\"),\n",
    "    dl.Layer(LayerNorm, 10).set_input_map(x=\"y\", batch=\"batch\") \\\n",
    "        .set_output_map(\"y\"),\n",
    "    dl.Add().set_input_map(\"x1\", \"y\").set_output_map(\"x1\"),\n",
    "    dl.Add().set_input_map(\"x2\", \"y\").set_output_map(\"x2\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above:\n",
    "\n",
    "1. First, a Bilinear layer combines inputs `x1` and `x2`, and the result is labeled as `y`. \n",
    "\n",
    "2. Then, a layer norm operation is applied to `y`, considering batch information from `batch`, and the output is newly assigned to `y`. \n",
    "\n",
    "3. Finally, `x1` and `x2` are independently summed with `y` and stored in the keys `x1` and `x2`, respectively, implementing a residual connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  <bound method BaseData.keys of Data(x1=[5, 10], x2=[5, 10], batch=[5], y=[5, 10])>\n",
      "output x1 shape:  torch.Size([5, 10])\n",
      "output x2 shape:  torch.Size([5, 10])\n",
      "output y shape:  torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "input_data = Data(\n",
    "    x1=torch.randn(5, 10),\n",
    "    x2=torch.randn(5, 10),\n",
    "    batch=torch.Tensor([0, 0, 0, 1, 1]).long(),\n",
    ")\n",
    "\n",
    "output_data = built(input_data)\n",
    "\n",
    "print(\"output keys: \", output_data.keys)\n",
    "print(\"output x1 shape: \", output_data.x1.shape)\n",
    "print(\"output x2 shape: \", output_data.x2.shape)\n",
    "print(\"output y shape: \", output_data.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Mappings to Tensor Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeplay supports a seamless transition between mapping objects and Tensor objects. The output data structure can be converted to a Tensor object using the `FromDict` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = dl.Sequential(\n",
    "    dl.Layer(torch.nn.Bilinear, 10, 10, 10).set_input_map(\"x1\", \"x2\") \\\n",
    "        .set_output_map(\"y\"),\n",
    "    dl.Layer(LayerNorm, 10).set_input_map(x=\"y\", batch=\"batch\") \\\n",
    "        .set_output_map(\"y\"),\n",
    "    dl.FromDict(\"y\"), \n",
    "    dl.Layer(torch.nn.Linear, 10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output type:  <class 'torch.Tensor'>\n",
      "output shape:  torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "input_data = Data(\n",
    "    x1=torch.randn(5, 10),\n",
    "    x2=torch.randn(5, 10),\n",
    "    batch=torch.Tensor([0, 0, 0, 1, 1]).long(),\n",
    ")\n",
    "\n",
    "output_data = built(input_data)\n",
    "\n",
    "print(\"output type: \", type(output_data))\n",
    "print(\"output shape: \", output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Parallel Pipelines\n",
    "\n",
    "Deeplay's `Parallel` method enables users to create parallel pipelines that can process input mapping objects concurrently. Each module in the pipeline receives the input data, extracts the relevant keys, and processes the data independently. Finally, the outputs from each module are merged into a single mapping object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  <bound method BaseData.keys of Data(x=[5, 10], y=[5, 30], z=[5, 30])>\n",
      "output y shape:  torch.Size([5, 30])\n",
      "output z shape:  torch.Size([5, 30])\n"
     ]
    }
   ],
   "source": [
    "module = dl.Parallel(\n",
    "    dl.Layer(torch.nn.Linear, 10, 30).set_input_map(\"x\").set_output_map(\"y\"),\n",
    "    dl.Layer(torch.nn.Linear, 10, 30).set_input_map(\"x\").set_output_map(\"z\"),\n",
    ")\n",
    "\n",
    "built = module.build()\n",
    "\n",
    "input_data = Data(x=torch.randn(5, 10))\n",
    "\n",
    "output_data = built(input_data)\n",
    "\n",
    "print(\"output keys: \", output_data.keys)\n",
    "print(\"output y shape: \", output_data.y.shape)\n",
    "print(\"output z shape: \", output_data.z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the output mappings can be specified directly within `Parallel`Â´s constructor. \n",
    "\n",
    "In the example below, each module is given a key (`y` and `z` for the first and second layer, respectively), and the output mappings are automatically determined based on these keys.\n",
    "\n",
    "```python\n",
    "module = dl.Parallel(\n",
    "    y=dl.Layer(torch.nn.Linear, 10, 30).set_input_map(\"x\"),\n",
    "    z=dl.Layer(torch.nn.Linear, 10, 30).set_input_map(\"x\"),\n",
    ")\n",
    "```\n",
    "\n",
    "This method streamlines the syntax but offers less explicit control over output mappings compared to individual module configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
