{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mappings as input to DeeplayModules**\n",
    "\n",
    "DeeplayModules support mapping objects (i.e., container objects that support arbitrary key lookups) as input, allowing users to define complex data processing pipelines in a simple and intuitive way.\n",
    "\n",
    "To use mappings as input to DeeplayModules, the user must define the mapping between the input and output data structures. This is done by configuring the `set_input_map` and `set_output_map` methods of the module.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Jesus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\Users\\Jesus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Layer[Linear](in_features=10, out_features=64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "import torch.nn as nn\n",
    "\n",
    "module = dl.Layer(nn.Linear, 10, 64)\n",
    "\n",
    "module.set_input_map(\"x\")\n",
    "module.set_output_map(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a Linear module that takes a dictionary with a key `x` as input and overwrites the key `x` with the output of the linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently supported mapping objects include dictionaries and Torch Geometric Data objects. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "input_data = Data(x=torch.randn(100, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the module is built, the input data can be passed to the module as with Tensor objects. The module will automatically map the input and output data to the expected data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  ['x']\n",
      "input x shape:  torch.Size([100, 10])\n",
      "output x shape:  torch.Size([100, 64])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "output_data = built(input_data)\n",
    "print(\"output keys: \", output_data.keys)\n",
    "print(\"input x shape: \", input_data.x.shape)\n",
    "print(\"output x shape: \", output_data.x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Basic operations with mappings**\n",
    "\n",
    "The `set_input_map` method supports local key re-assigment, useful when the input data key doesn't match the expected key by the module. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[MultiheadAttention](embed_dim=10, num_heads=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = dl.Layer(nn.MultiheadAttention, embed_dim=10, num_heads=2)\n",
    "\n",
    "module.set_input_map(query=\"x\", key=\"x\", value=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, `set_output_map` method receives an second argument to specify the mapping for the attention weights output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[MultiheadAttention](embed_dim=10, num_heads=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.set_output_map(\"x\", \"attention_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[MultiheadAttention](embed_dim=10, num_heads=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.set_output_map(x=0, attention_weights=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where, `x=0` and `attention_weights=1` indicate that `x` and `attention_weights` in the output dictionary correspond to the first and second outputs of the module, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  ['attention_weights', 'x']\n",
      "output x shape:  torch.Size([100, 10])\n",
      "output attention_weights shape:  torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "input_data = Data(x=torch.randn(100, 10))\n",
    "\n",
    "output_data = built(input_data)\n",
    "print(\"output keys: \", output_data.keys)\n",
    "print(\"output x shape: \", input_data.x.shape)\n",
    "print(\"output attention_weights shape: \", output_data.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New keys can be added to the output data structure to store intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer[MultiheadAttention](embed_dim=10, num_heads=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = dl.Layer(nn.MultiheadAttention, embed_dim=10, num_heads=2)\n",
    "\n",
    "\n",
    "module.set_input_map(query=\"x\", key=\"x\", value=\"x\")\n",
    "\n",
    "module.set_output_map(\n",
    "    x=0,\n",
    "    intermediate_x=0,\n",
    "    attention_weights=1,\n",
    "    intermediate_attention_weights=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  ['intermediate_x', 'attention_weights', 'intermediate_attention_weights', 'x']\n",
      "output x shape:  torch.Size([100, 10])\n",
      "output intermediate_x shape:  torch.Size([100, 10])\n",
      "output attention_weights shape:  torch.Size([100, 100])\n",
      "output intermediate_attention_weights shape:  torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "input_data = Data(x=torch.randn(100, 10))\n",
    "\n",
    "output_data = built(input_data)\n",
    "print(\"output keys: \", output_data.keys)\n",
    "\n",
    "print(\"output x shape: \", input_data.x.shape)\n",
    "print(\"output intermediate_x shape: \", output_data.intermediate_x.shape)\n",
    "\n",
    "print(\"output attention_weights shape: \", output_data.attention_weights.shape)\n",
    "print(\n",
    "    \"output intermediate_attention_weights shape: \",\n",
    "    output_data.intermediate_attention_weights.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mappings allow sequential processing**\n",
    "\n",
    "Mapping enables the sequencing of branched pipelines, fully exploiting the modular design of DeeplayModules.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import LayerNorm\n",
    "\n",
    "module = dl.Sequential(\n",
    "    dl.Layer(nn.Bilinear, 10, 10, 10).set_input_map(\"x1\", \"x2\").set_output_map(\"y\"),\n",
    "    dl.Layer(LayerNorm, 10).set_input_map(x=\"y\", batch=\"batch\").set_output_map(\"y\"),\n",
    "    dl.Add().set_input_map(\"x1\", \"y\").set_output_map(\"x1\"),\n",
    "    dl.Add().set_input_map(\"x2\", \"y\").set_output_map(\"x2\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above,\n",
    "\n",
    "First, a Bilinear layer combines inputs `x1` and `x2`, and the result is labeled as `y`. \n",
    "\n",
    "Then, a layer norm operation is applied to `y`, considering batch information from `batch`, and the output is newly assigned to `y`. \n",
    "\n",
    "Finally, `x1` and `x2` are independently summed with `y` and stored in the keys `x1` and `x2`, respectively, implementing a residual connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  ['x2', 'batch', 'y', 'x1']\n",
      "output x1 shape:  torch.Size([5, 10])\n",
      "output x2 shape:  torch.Size([5, 10])\n",
      "output y shape:  torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "input_data = Data(\n",
    "    x1=torch.randn(5, 10),\n",
    "    x2=torch.randn(5, 10),\n",
    "    batch=torch.Tensor([0, 0, 0, 1, 1]).long(),\n",
    ")\n",
    "\n",
    "output_data = built(input_data)\n",
    "print(\"output keys: \", output_data.keys)\n",
    "print(\"output x1 shape: \", output_data.x1.shape)\n",
    "print(\"output x2 shape: \", output_data.x2.shape)\n",
    "print(\"output y shape: \", output_data.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **From mappings to Tensor objects**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeplay supports a seamless transition between mapping objects and Tensor objects. The output data structure can be converted to a Tensor object using the `FromDict` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = dl.Sequential(\n",
    "    dl.Layer(nn.Bilinear, 10, 10, 10).set_input_map(\"x1\", \"x2\").set_output_map(\"y\"),\n",
    "    dl.Layer(LayerNorm, 10).set_input_map(x=\"y\", batch=\"batch\").set_output_map(\"y\"),\n",
    "    dl.FromDict(\"y\"), \n",
    "    dl.Layer(nn.Linear, 10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output type:  <class 'torch.Tensor'>\n",
      "output shape:  torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "built = module.build()\n",
    "\n",
    "input_data = Data(\n",
    "    x1=torch.randn(5, 10),\n",
    "    x2=torch.randn(5, 10),\n",
    "    batch=torch.Tensor([0, 0, 0, 1, 1]).long(),\n",
    ")\n",
    "\n",
    "output_data = built(input_data)\n",
    "print(\"output type: \", type(output_data))\n",
    "print(\"output shape: \", output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining parallel pipelines**\n",
    "\n",
    "Deeplay's `Parallel` method enables users to create parallel pipelines that can process input mapping objects concurrently. Each module in the pipeline receives the input data, extracts the relevant keys, and processes the data independently. Finally, the outputs from each module are merged into a single mapping object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys:  ['z', 'y', 'x']\n",
      "output y shape:  torch.Size([5, 30])\n",
      "output z shape:  torch.Size([5, 30])\n"
     ]
    }
   ],
   "source": [
    "module = dl.Parallel(\n",
    "    dl.Layer(nn.Linear, 10, 30).set_input_map(\"x\").set_output_map(\"y\"),\n",
    "    dl.Layer(nn.Linear, 10, 30).set_input_map(\"x\").set_output_map(\"z\"),\n",
    ")\n",
    "\n",
    "built = module.build()\n",
    "\n",
    "input_data = Data(x=torch.randn(5, 10))\n",
    "\n",
    "output_data = built(input_data)\n",
    "print(\"output keys: \", output_data.keys)\n",
    "print(\"output y shape: \", output_data.y.shape)\n",
    "print(\"output z shape: \", output_data.z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the output mappings can be specified directly within `Parallel`´s constructor. \n",
    "\n",
    "In the example below, each module is given a key (`y` and `z` for the first and second layer, respectively), and the output mappings are automatically determined based on these keys.\n",
    "\n",
    "```python\n",
    "module = dl.Parallel(\n",
    "    y=dl.Layer(nn.Linear, 10, 30).set_input_map(\"x\"),\n",
    "    z=dl.Layer(nn.Linear, 10, 30).set_input_map(\"x\"),\n",
    ")\n",
    "```\n",
    "\n",
    "This method streamlines the syntax but offers less explicit control over output mappings compared to individual module configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
