{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Woring with Deeplay Applications\n",
        "\n",
        "Applications are broadly defined as classes that represent a task, such as classification or regression, without depending heavily on the exact architecture. They are the highest level of abstraction in the Deeplay library. Applications are designed to be easy to use and require minimal configuration to get started. They are also designed to be easily extensible, so that you can add new features without having to modify the existing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is There in an Application?\n",
        "\n",
        "As a general rule of thumb, try to minimize the number of models in an application. Best is if there is a single model, accessed as `app.model`. \n",
        "\n",
        "Some applications require more,\n",
        "such as `gan.generator` and `gan.discriminator`. This is fine, but try to keep it to a minimum. \n",
        "\n",
        "A bad example would be for a classifier to include `app.conv_backbone`, `app.conv_to_fc_connector` and `app.fc_head`. This is bad because it limits the flexibility of the application to architectures that fit this exact structure. Instead, the application should have a single model that can be easily replaced with a different one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining How to Train an Application\n",
        "\n",
        "The primary function of an application is to define how it is trained. This includes the loss function, the optimizer, and the metrics that are used to evaluate the model. Applications also define how the model is trained, including the training loop, the validation loop, and the testing loop. Applications are designed to be easy to use, so that you can get started quickly without having to worry about the details of the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining a Standard Training\n",
        "\n",
        "The training step is, broadly, defined as follows:\n",
        "\n",
        "```python\n",
        "x, y = self.train_preprocess(batch)\n",
        "y_hat = self(x)  # Call forward method.\n",
        "loss = self.compute_loss(y_hat, y)\n",
        "# logging\n",
        "```\n",
        "\n",
        "If the training can be defined in this way, then you can straightforwardly implement the `.train_preprocess()`, `.forward()`, and `.compute_loss()` methods to define the training process.\n",
        "\n",
        "The default behavior of `.train_preprocess()` is the identity function. The `.train_preprocess()` method is intended to apply any operations that cannot be simply defined as a part of the dataset. For example, in some self-supervised models, the target is calculated from the input data. This can be done here. It can also be used to ensure the dtype of the input matches the expected dtype of the model. Most likely, you will not need to override this method.\n",
        "\n",
        "The default behavior of `.compute_loss()` is to call `self.loss(y_hat, y)`. The `.compute_loss()` method is intended to calculate the loss of the model. This can be as simple as calling `self.loss(y_hat, y)`, or it can be more complex. It is more likely that you will need to override this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining a More Complex Training\n",
        "\n",
        "If the training process is more complex and you need to define a custom training loop, you can override the `training_step` method entirely. This method is called for each batch of data during training. It should return the loss for the batch. \n",
        "\n",
        "Note that if you override the `.training_step()` method, you will have to handle the logging of the loss yourself. This is done by calling `self.log(\"train_loss\", loss, ...)` where `...` is any setting you want to pass to the logger (see `lightning.LightningModule.log()` for more information)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
