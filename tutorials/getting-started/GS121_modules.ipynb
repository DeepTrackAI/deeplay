{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Deeplay Modules\n",
    "\n",
    "In this section, you'll learn how to create and build Deeplay modules as well as how to configure their properties. You'll also understand the difference between Deeplay and PyTorch modules. You'll "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Differences between Deeplay and PyTorch Modules\n",
    "\n",
    "The biggest difference between a Deeplay module and a PyTorch module is that the Deeplay module is not immediately fully initialized. This is to allow the user to further configure it before the module is built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Building Deeplay Modules\n",
    "\n",
    "Deeplay modules can be built using either `.create()` or `.build()`. \n",
    "\n",
    "Use the `.create()` method when you want to keep the original module for subsequent use and/or modification. \n",
    "\n",
    "Use the `.build()` method when you want to build the module in-place. \n",
    "\n",
    "Define a Deeplay module ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=784, out_features=32, bias=True)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "      (normalization): Layer[BatchNorm1d](num_features=32)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=32, bias=True)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "      (normalization): Layer[BatchNorm1d](num_features=32)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=10, bias=True)\n",
      "      (activation): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "mlp = dl.models.SmallMLP(in_features=784, out_features=10)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... this module is not built yet. For example, this can be seen in the summary by the existence of Deeplay `Layer` objects followed by the underlying PyTorch layer in square brackets. Once the module is built, the `Layer` objects are replaced by the actual PyTorch layers. \n",
    "\n",
    "Start by creating the `mlp` module ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp=\n",
      " SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=784, out_features=32, bias=True)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "      (normalization): Layer[BatchNorm1d](num_features=32)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=32, bias=True)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "      (normalization): Layer[BatchNorm1d](num_features=32)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=10, bias=True)\n",
      "      (activation): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "created_mlp=\n",
      " SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=784, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=10, bias=True)\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "created_mlp = mlp.create()\n",
    "\n",
    "print(\"mlp=\\n\", mlp)\n",
    "print(\"created_mlp=\\n\", created_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `created_mlp` is a new module, while `mlp` is the same module as before.\n",
    "\n",
    "Now, build the `mlp` module ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp=\n",
      " SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=784, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=10, bias=True)\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "built_mlp=\n",
      " SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=784, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=10, bias=True)\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "built_mlp = mlp.build()\n",
    "\n",
    "print(\"mlp=\\n\", mlp)\n",
    "print(\"built_mlp=\\n\", built_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... you can see that both `mlp` and `built_mlp` are now built. In fact, they are the same object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp is built_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding Whether to Use `.build()` or `.create()`\n",
    "\n",
    "In general, you'll want to use `.build()` when you are sure you won't need the original module anymore, and `.create()` when you want to keep the original template (for example, when you want to create multiple similar modules). Most of the time, you'll probably want to use `.build()`.\n",
    "\n",
    "The `.create()` method is actually equivalent to `.new().build()`. This is because `.new()` clones the object, and `.build()` builds the object in-place. The `.new()` method can also be used by itself to create a clone of the object without building it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with PyTorch Tensors\n",
    "\n",
    "Deeplay is compatible with both NumPy arrays and PyTorch tensors. However, internally, when a NumPy tensor is passed to the model, it is converted to a PyTorch tensor. This is because PyTorch only works with PyTorch tensors. This conversion also moves the channel dimension of the tensor from the last dimension to the first non-batch dimension (as is expected by PyTorch). \n",
    "\n",
    "**NOTE:** While Deeplay takes all possible care to to ensure that this is done correctly, it is generally recommend directly providing PyTorch tensors to avoid any automatic permuting of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Modules\n",
    "\n",
    "Deeplay modules have a configuration system that allows you to easily change the properties of a module. At its core, this is done using the `.configure()` method. However, most modules also have specific configuration methods that allow you to change specific properties. For example, the `LinearBlock` has the `.normalized()` and `.activated()` methods that allow you to add normalization and activation to the block.\n",
    "\n",
    "Importantly, most configurations are applied to many layers at once. For example, you may want all blocks in a component to have the same activation function. There are a few ways to do this, but the most powerful of all is the selection system. This will be more thoroughly explained in [GS181 Configuring Deeplay Objects](GS181_configure.ipynb), but the basic idea is that you can select a subset of layers in a module and apply a configuration to them. This is done using the `.__getitem__()` method. For example, to apply an activation function to all blocks in a component, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=728, out_features=32, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=16, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=16, out_features=10, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "mlp[\"blocks\", :].all.configure(activation=dl.Layer(nn.Tanh))\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are a few ways to achieve the same configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=728, out_features=32, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=16, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=16, out_features=10, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "mlp[\"blocks\", :].all.activated(nn.Tanh)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=728, out_features=32, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=16, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=16, out_features=10, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "mlp[...].hasattr(\"activated\").all.activated(nn.Tanh)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=728, out_features=32, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=16, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=16, out_features=10, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "mlp[...].isinstance(dl.LinearBlock).all.activated(nn.Tanh)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=728, out_features=32, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=16, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=16, out_features=10, bias=True)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "for block in mlp.blocks:\n",
    "    block.activated(nn.Tanh)\n",
    "    \n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many such methods, and they are usually composable. For example, let's say you want a block that first applies the layer and the activation, then has an additive shortcut connection from the input, and finally applies a normalization. You can do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearBlock(\n",
      "  (shortcut_start): Identity()\n",
      "  (layer): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (activation): GELU(approximate='none')\n",
      "  (shortcut_end): Add()\n",
      "  (normalization): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block = (\n",
    "    dl.LinearBlock(64, 64)\n",
    "    .activated(nn.GELU)\n",
    "    .shortcut()\n",
    "    .normalized(nn.LayerNorm)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very powerful system that allows you to easily create complex blocks and components. It can of course be used on blocks inside of components or models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (shortcut_start): Layer[Identity]()\n",
      "      (layer): Layer[Linear](in_features=784, out_features=64, bias=True)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (shortcut_end): Add()\n",
      "      (normalization): Layer[LayerNorm](normalized_shape=64)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (shortcut_start): Layer[Identity]()\n",
      "      (layer): Layer[Linear](in_features=64, out_features=64, bias=True)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (shortcut_end): Add()\n",
      "      (normalization): Layer[LayerNorm](normalized_shape=64)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=64, out_features=10, bias=True)\n",
      "      (activation): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = dl.MultiLayerPerceptron(784, [64, 64], 10)\n",
    "model[\"blocks\", :-1] \\\n",
    "    .all \\\n",
    "    .activated(nn.ReLU) \\\n",
    "    .shortcut() \\\n",
    "    .normalized(nn.LayerNorm)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Modules with Styles\n",
    "\n",
    "Some special configurations of modules have been given names, and can be applied using the `.style()` method. For example, the `Conv2dBlock` has a `\"resnet\"` style that applies the resnet style residual connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2dBlock(\n",
      "  (blocks): Sequential(\n",
      "    (0-1): 2 x Conv2dBlock(\n",
      "      (shortcut_start): Conv2dBlock(\n",
      "        (layer): Identity()\n",
      "        (activation): Identity()\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): Conv2dBlock(\n",
      "          (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1): Conv2dBlock(\n",
      "          (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (shortcut_end): Add()\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "convblock = dl.Conv2dBlock(64, 64).style(\"resnet\").build()\n",
    "\n",
    "print(convblock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same holds for components and models. For example, the `ConvolutionalEncoder2d` has a `\"resnet18\"` style that applies the resnet style residual connection to all blocks in the component, and styles the input and output blocks to match the resnet18 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
