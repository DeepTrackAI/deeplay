{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Deeplay Modules\n",
    "\n",
    "In this section, you'll explore the difference between Deeplay and PyTorch modules. You'll learn how to create and build Deeplay modules as well as how to configure their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Differences between Deeplay and PyTorch Modules\n",
    "\n",
    "The biggest difference between a Deeplay module and a PyTorch module is that the Deeplay module is not immediately fully initialized. This is to allow the user to insert any configuration they want before the module is built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Building Deeplay Modules\n",
    "\n",
    "Deeplay modules can be built using either `.create()` or `.build()`. The former is used when you want to keep the original module for subsequent use and/or modification, while the latter is used when you want to build the module in-place. \n",
    "\n",
    "Create a Deeplay module ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deeplay.models' has no attribute 'SmallMLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdeeplay\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdl\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m mlp \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSmallMLP\u001b[49m(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(mlp)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deeplay.models' has no attribute 'SmallMLP'"
     ]
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "mlp = dl.models.SmallMLP(in_features=784, out_features=10)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... this module is not built yet. This can be seen in the summary by the existence of `Layer` objects. Once the module is built, the `Layer` objects are replaced by the actual PyTorch layers. \n",
    "\n",
    "Start by creating the `mlp` module ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=784, out_features=32, bias=True)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "      (normalization): Layer[BatchNorm1d](num_features=32)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=32, bias=True)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "      (normalization): Layer[BatchNorm1d](num_features=32)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=10, bias=True)\n",
      "      (activation): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Created SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=784, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=10, bias=True)\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "created = mlp.create()\n",
    "\n",
    "print(\"mlp=\\n\", mlp)\n",
    "print(\"created=\\n\", created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `created` is a new module, while `mlp` is the same module as before.\n",
    "\n",
    "Now, build the mlp module ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=728, out_features=32, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=16, out_features=10, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Built MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=728, out_features=32, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=16, out_features=10, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "built = mlp.build()\n",
    "\n",
    "print(\"mlp=\\n\", mlp)\n",
    "print(\"built=\\n\", built)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... you can see that both `mlp` and `built` are now built. In fact, they are the same object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp is built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding Whether to Use `.build()` or `.create()`\n",
    "\n",
    "In general, you'll want to use `.build()` when you are sure you won't need the original module anymore, and `.create()` when you want to keep the original template (for example, when you want to create multiple similar modules). Most of the time, you'll want to use `.build()`.\n",
    "\n",
    "The `.create()` method is actually equivalent to `.new().build()`. This is because `.new()` creates a new identical object, and `.build()` builds the object in-place. The `.new()` method can also be used by itself to create a new object without building it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with PyTorch Tensors\n",
    "\n",
    "Deeplay is compatible with both NumPy arrays and PyTorch tensors. However, internally, when a NumPy tensor is passed to the model, it is converted to a PyTorch tensor. This is because PyTorch only works with PyTorch tensors. This conversion also moves the channel dimension of the tensor from the last dimension to the first non-batch dimension (as is expected by torch). \n",
    "\n",
    "**Note:** While Deeplay takes all possible care to to ensure that this is done correctly, it is generally recommend directly providing PyTorch tensors to avoid any automatic permuting of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Modules\n",
    "\n",
    "Another major difference is the configuration system. Deeplay modules have a configuration system that allows you to easily change the properties of a module. At its core, this is done using the `.configure()` method. However, most modules also have specific configuration methods that allow you to change specific properties. For example, the `LinearBlock` has the `normalized()` and `activated()` methods that allow you to add normalization and activation to the block.\n",
    "\n",
    "Importantly, most configurations are applied to many layers at once. For example, you may want all blocks in a component to have the same activation function. There are a few ways to do this, but the most powerful of all is the selection system. This will be more thoroughly explained in a [subsequent notebook](link), but the basic idea is that you can select a subset of layers in a module and apply a configuration to them. This is done using the `__getitem__()` method. For example, to apply an activation function to all blocks in a component, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=728, out_features=32)\n",
      "      (activation): Layer[Tanh]()\n",
      "      (normalization): Layer[Identity](num_features=32)\n",
      "      (dropout): Layer[Dropout](p=0)\n",
      "    )\n",
      "    (1): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=16)\n",
      "      (activation): Layer[Tanh]()\n",
      "      (normalization): Layer[Identity](num_features=16)\n",
      "      (dropout): Layer[Dropout](p=0)\n",
      "    )\n",
      "    (2): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=16, out_features=10)\n",
      "      (activation): Layer[Tanh]()\n",
      "      (normalization): Layer[Identity](num_features=10)\n",
      "      (dropout): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "mlp[\"blocks\", :].all.configure(activation=dl.Layer(nn.Tanh))\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are a few ways to achieve the same configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Module LayerActivationNormalizationDropout(\n  (layer): Layer[Linear](in_features=728, out_features=32)\n  (activation): Layer[Tanh]()\n  (normalization): Layer[Identity](num_features=32)\n  (dropout): Layer[Dropout](p=0)\n) does not have a method activated. Use selection.hasattr('method_name') to filter modules that have the method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/deeplay/deeplay/module.py:1290\u001b[0m, in \u001b[0;36m_MethodForwarder._create_forwarder.<locals>.forwarder\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1290\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LayerActivationNormalizationDropout' object has no attribute 'activated'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTanh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(mlp)\n",
      "File \u001b[0;32m~/Documents/GitHub/deeplay/deeplay/module.py:1294\u001b[0m, in \u001b[0;36m_MethodForwarder._create_forwarder.<locals>.forwarder\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have a method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse selection.hasattr(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) to filter modules that have the method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1297\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Module LayerActivationNormalizationDropout(\n  (layer): Layer[Linear](in_features=728, out_features=32)\n  (activation): Layer[Tanh]()\n  (normalization): Layer[Identity](num_features=32)\n  (dropout): Layer[Dropout](p=0)\n) does not have a method activated. Use selection.hasattr('method_name') to filter modules that have the method."
     ]
    }
   ],
   "source": [
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "mlp[\"blocks\", :].all.activated(nn.Tanh)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=728, out_features=32)\n",
      "      (activation): Layer[Tanh]()\n",
      "      (normalization): Layer[Identity](num_features=32)\n",
      "      (dropout): Layer[Dropout](p=0)\n",
      "    )\n",
      "    (1): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=16)\n",
      "      (activation): Layer[Tanh]()\n",
      "      (normalization): Layer[Identity](num_features=16)\n",
      "      (dropout): Layer[Dropout](p=0)\n",
      "    )\n",
      "    (2): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=16, out_features=10)\n",
      "      (activation): Layer[Tanh]()\n",
      "      (normalization): Layer[Identity](num_features=10)\n",
      "      (dropout): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "mlp[...].hasattr(\"activated\").all.activated(nn.Tanh)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "mlp[...].isinstance(dl.LinearBlock).all.activated(nn.Tanh)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = dl.MultiLayerPerceptron(728, [32, 16], 10)\n",
    "for block in mlp.blocks:\n",
    "    block.activated(nn.Tanh)\n",
    "    \n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many such methods, and they are usually composable. For example, let's say you want a block that first applies the layer and the activationm, then has an additive shortcut connection from the input, and finally applies a normalization. You can do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deeplay' has no attribute 'LinearBlock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m block \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinearBlock\u001b[49m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mactivated(nn\u001b[38;5;241m.\u001b[39mGELU)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mshortcut()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mnormalized(nn\u001b[38;5;241m.\u001b[39mLayerNorm)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(block)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deeplay' has no attribute 'LinearBlock'"
     ]
    }
   ],
   "source": [
    "block = (\n",
    "    dl.LinearBlock(64, 64)\n",
    "    .activated(nn.GELU)\n",
    "    .shortcut()\n",
    "    .normalized(nn.LayerNorm)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very powerful system that allows you to easily create complex blocks and components. It can of course be used on blocks inside of components or models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Module LayerActivationNormalizationDropout(\n  (layer): Layer[Linear](in_features=784, out_features=64)\n  (activation): Layer[ReLU]()\n  (normalization): Layer[Identity](num_features=64)\n  (dropout): Layer[Dropout](p=0)\n) does not have a method activated. Use selection.hasattr('method_name') to filter modules that have the method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/deeplay/deeplay/module.py:1290\u001b[0m, in \u001b[0;36m_MethodForwarder._create_forwarder.<locals>.forwarder\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1290\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LayerActivationNormalizationDropout' object has no attribute 'activated'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m dl\u001b[38;5;241m.\u001b[39mMultiLayerPerceptron(\u001b[38;5;241m784\u001b[39m, [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m], \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReLU\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mshortcut() \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mnormalized(nn\u001b[38;5;241m.\u001b[39mLayerNorm)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/Documents/GitHub/deeplay/deeplay/module.py:1294\u001b[0m, in \u001b[0;36m_MethodForwarder._create_forwarder.<locals>.forwarder\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have a method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse selection.hasattr(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) to filter modules that have the method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1297\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Module LayerActivationNormalizationDropout(\n  (layer): Layer[Linear](in_features=784, out_features=64)\n  (activation): Layer[ReLU]()\n  (normalization): Layer[Identity](num_features=64)\n  (dropout): Layer[Dropout](p=0)\n) does not have a method activated. Use selection.hasattr('method_name') to filter modules that have the method."
     ]
    }
   ],
   "source": [
    "model = dl.MultiLayerPerceptron(784, [64, 64], 10)\n",
    "model[\"blocks\", :-1] \\\n",
    "    .all \\\n",
    "    .activated(nn.ReLU) \\\n",
    "    .shortcut() \\\n",
    "    .normalized(nn.LayerNorm)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Modules with Styles\n",
    "\n",
    "Some special configurations of modules have been given names, and can be applied using the `style()` method. For example, the `Conv2dBlock` has a `resnet` style that applies the resnet style residual connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2dBlock(\n",
       "  (blocks): Sequential(\n",
       "    (0): Conv2dBlock(\n",
       "      (shortcut_start): Conv2dBlock(\n",
       "        (layer): Identity()\n",
       "        (activation): Identity()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): Conv2dBlock(\n",
       "          (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (1): Conv2dBlock(\n",
       "          (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut_end): Add()\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1): Conv2dBlock(\n",
       "      (shortcut_start): Conv2dBlock(\n",
       "        (layer): Identity()\n",
       "        (activation): Identity()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): Conv2dBlock(\n",
       "          (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (1): Conv2dBlock(\n",
       "          (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut_end): Add()\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convblock = dl.Conv2dBlock(64, 64).style(\"resnet\").build()\n",
    "\n",
    "print(convblock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same holds for components and models. For example, the `ConvolutionalEncoder2d` has a `resnet18` style that applies the resnet style residual connection to all blocks in the component, and styles the input and output blocks to match the resnet18 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
