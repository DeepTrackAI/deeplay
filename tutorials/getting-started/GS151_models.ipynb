{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deeplay Models\n",
    "\n",
    "In Deeplay, models are typically complete neural networks that don't need any further customization to be used for a specific complete application or as a substantial part of it (for example, the backbone models that usually need a head to become useful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of Models Available in Deeplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18\n",
    "\n",
    "The `ResNet18` is available as a backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackboneResnet18(\n",
      "  (blocks): LayerList(\n",
      "    (0): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
      "      (normalization): Layer[BatchNorm2d](num_features=64)\n",
      "      (activation): Layer[ReLU](inplace=True)\n",
      "      (pool): Layer[MaxPool2d](kernel_size=3, stride=2, padding=1, ceil_mode=False, dilation=1)\n",
      "    )\n",
      "    (1): Conv2dBlock(\n",
      "      (blocks): Sequential(\n",
      "        (0-1): 2 x Conv2dBlock(\n",
      "          (shortcut_start): Conv2dBlock(\n",
      "            (layer): Layer[Identity](in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=64)\n",
      "              (activation): Layer[ReLU]()\n",
      "            )\n",
      "            (1): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=64)\n",
      "            )\n",
      "          )\n",
      "          (shortcut_end): Add()\n",
      "          (activation): Layer[ReLU]()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Conv2dBlock(\n",
      "      (blocks): Sequential(\n",
      "        (0): Conv2dBlock(\n",
      "          (shortcut_start): Conv2dBlock(\n",
      "            (layer): Layer[Conv2d](in_channels=64, out_channels=128, kernel_size=1, stride=2, padding=0)\n",
      "            (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "              (activation): Layer[ReLU]()\n",
      "            )\n",
      "            (1): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "            )\n",
      "          )\n",
      "          (shortcut_end): Add()\n",
      "          (activation): Layer[ReLU]()\n",
      "        )\n",
      "        (1): Conv2dBlock(\n",
      "          (shortcut_start): Conv2dBlock(\n",
      "            (layer): Layer[Identity](in_channels=128, out_channels=128, kernel_size=1, stride=1, padding=0)\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "              (activation): Layer[ReLU]()\n",
      "            )\n",
      "            (1): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "            )\n",
      "          )\n",
      "          (shortcut_end): Add()\n",
      "          (activation): Layer[ReLU]()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Conv2dBlock(\n",
      "      (blocks): Sequential(\n",
      "        (0): Conv2dBlock(\n",
      "          (shortcut_start): Conv2dBlock(\n",
      "            (layer): Layer[Conv2d](in_channels=128, out_channels=256, kernel_size=1, stride=2, padding=0)\n",
      "            (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "              (activation): Layer[ReLU]()\n",
      "            )\n",
      "            (1): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "            )\n",
      "          )\n",
      "          (shortcut_end): Add()\n",
      "          (activation): Layer[ReLU]()\n",
      "        )\n",
      "        (1): Conv2dBlock(\n",
      "          (shortcut_start): Conv2dBlock(\n",
      "            (layer): Layer[Identity](in_channels=256, out_channels=256, kernel_size=1, stride=1, padding=0)\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "              (activation): Layer[ReLU]()\n",
      "            )\n",
      "            (1): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "            )\n",
      "          )\n",
      "          (shortcut_end): Add()\n",
      "          (activation): Layer[ReLU]()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Conv2dBlock(\n",
      "      (blocks): Sequential(\n",
      "        (0): Conv2dBlock(\n",
      "          (shortcut_start): Conv2dBlock(\n",
      "            (layer): Layer[Conv2d](in_channels=256, out_channels=512, kernel_size=1, stride=2, padding=0)\n",
      "            (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "              (activation): Layer[ReLU]()\n",
      "            )\n",
      "            (1): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "            )\n",
      "          )\n",
      "          (shortcut_end): Add()\n",
      "          (activation): Layer[ReLU]()\n",
      "        )\n",
      "        (1): Conv2dBlock(\n",
      "          (shortcut_start): Conv2dBlock(\n",
      "            (layer): Layer[Identity](in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0)\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "              (activation): Layer[ReLU]()\n",
      "            )\n",
      "            (1): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "            )\n",
      "          )\n",
      "          (shortcut_end): Add()\n",
      "          (activation): Layer[ReLU]()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (postprocess): Layer[Identity]()\n",
      "  (pool): Layer[AdaptiveAvgPool2d](output_size=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet18 = dl.models.BackboneResnet18(in_channels=3, pool_output=True)\n",
    "\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGANGenerator(\n",
      "  (blocks): LayerList(\n",
      "    (0): Conv2dBlock(\n",
      "      (layer): Layer[ConvTranspose2d](in_channels=100, out_channels=1024, kernel_size=4, stride=1, padding=0)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (normalization): Layer[BatchNorm2d](num_features=1024)\n",
      "    )\n",
      "    (1): Conv2dBlock(\n",
      "      (layer): Layer[ConvTranspose2d](in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "    )\n",
      "    (2): Conv2dBlock(\n",
      "      (layer): Layer[ConvTranspose2d](in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "    )\n",
      "    (3): Conv2dBlock(\n",
      "      (layer): Layer[ConvTranspose2d](in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "    )\n",
      "    (4): Conv2dBlock(\n",
      "      (layer): Layer[ConvTranspose2d](in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[Tanh]()\n",
      "    )\n",
      "  )\n",
      "  (preprocess): Layer[Identity]()\n",
      "  (label_embedding): Layer[Identity]()\n",
      ")\n",
      "DCGANDiscriminator(\n",
      "  (blocks): LayerList(\n",
      "    (0): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "    )\n",
      "    (1): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "      (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "    )\n",
      "    (2): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "      (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "    )\n",
      "    (3): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "      (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "    )\n",
      "    (4): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=512, out_channels=1, kernel_size=4, stride=2, padding=0)\n",
      "      (activation): Layer[Sigmoid]()\n",
      "    )\n",
      "  )\n",
      "  (postprocess): Layer[Identity]()\n",
      "  (label_embedding): Layer[Identity]()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator = dl.models.DCGANGenerator(out_channels=3)\n",
    "discriminator = dl.models.DCGANDiscriminator(in_channels=3)\n",
    "\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CycleGANResnetGenerator(\n",
      "  (encoder): ConvolutionalEncoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Layer[Conv2d](in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3, padding_mode=reflect)\n",
      "        (normalization): Layer[InstanceNorm2d](num_features=64)\n",
      "        (activation): Layer[ReLU]()\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (layer): Layer[Conv2d](in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
      "        (normalization): Layer[InstanceNorm2d](num_features=128)\n",
      "        (activation): Layer[ReLU]()\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (layer): Layer[Conv2d](in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
      "        (normalization): Layer[InstanceNorm2d](num_features=256)\n",
      "        (activation): Layer[ReLU]()\n",
      "      )\n",
      "    )\n",
      "    (postprocess): Layer[Identity]()\n",
      "  )\n",
      "  (bottleneck): ConvolutionalNeuralNetwork(\n",
      "    (blocks): LayerList(\n",
      "      (0-8): 9 x Conv2dBlock(\n",
      "        (shortcut_start): Conv2dBlock(\n",
      "          (layer): Layer[Identity](in_channels=256, out_channels=256, kernel_size=1, stride=1, padding=0)\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0-1): 2 x Conv2dBlock(\n",
      "            (layer): Layer[Conv2d](in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
      "            (normalization): Layer[InstanceNorm2d](num_features=256)\n",
      "            (activation): Layer[ReLU]()\n",
      "          )\n",
      "        )\n",
      "        (shortcut_end): Add()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ConvolutionalDecoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Layer[ConvTranspose2d](in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
      "        (normalization): Layer[InstanceNorm2d](num_features=128)\n",
      "        (activation): Layer[ReLU]()\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (layer): Layer[ConvTranspose2d](in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
      "        (normalization): Layer[InstanceNorm2d](num_features=64)\n",
      "        (activation): Layer[ReLU]()\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (layer): Layer[Conv2d](in_channels=64, out_channels=3, kernel_size=7, stride=1, padding=3, padding_mode=reflect)\n",
      "        (activation): Layer[Tanh]()\n",
      "      )\n",
      "    )\n",
      "    (preprocess): Layer[Identity]()\n",
      "  )\n",
      ")\n",
      "CycleGANDiscriminator(\n",
      "  (blocks): LayerList(\n",
      "    (0): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "    )\n",
      "    (1): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
      "      (normalization): Layer[InstanceNorm2d](num_features=128)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "    )\n",
      "    (2): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
      "      (normalization): Layer[InstanceNorm2d](num_features=256)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "    )\n",
      "    (3): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1)\n",
      "      (normalization): Layer[InstanceNorm2d](num_features=512)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "    )\n",
      "    (4): Conv2dBlock(\n",
      "      (layer): Layer[Conv2d](in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1)\n",
      "      (activation): Layer[Sigmoid]()\n",
      "    )\n",
      "  )\n",
      "  (postprocess): Layer[Identity]()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator = dl.models.CycleGANResnetGenerator(in_channels=3, out_channels=3)\n",
    "discriminator = dl.models.CycleGANDiscriminator(in_channels=3)\n",
    "\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptrons\n",
    "\n",
    "Multi-layer perceptrons are available in various sizes.\n",
    "They can be used both as stand-alone applications and as backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=10, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=32, out_features=1, bias=True)\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "small = dl.models.SmallMLP(in_features=10, out_features=1).create()\n",
    "\n",
    "print(small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediumMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=10, out_features=64, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "medium = dl.models.MediumMLP(in_features=10, out_features=1).create()\n",
    "\n",
    "print(medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LargeMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=10, out_features=128, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1-2): 2 x LinearBlock(\n",
      "      (layer): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): LinearBlock(\n",
      "      (layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "large = dl.models.LargeMLP(in_features=10, out_features=1).create()\n",
    "\n",
    "print(large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLargeMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Linear(in_features=10, out_features=128, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): LinearBlock(\n",
      "      (layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.05)\n",
      "      (normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): LinearBlock(\n",
      "      (layer): Linear(in_features=512, out_features=1, bias=True)\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "xlarge = dl.models.XLargeMLP(in_features=10, out_features=1).create()\n",
    "\n",
    "print(xlarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network\n",
    "\n",
    "The `RecurrentModel` provides a base for recurrent neural networks. Specific implementations will be provided in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentModel(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (embedding_dropout): Dropout(p=0, inplace=False)\n",
      "  (blocks): LayerList(\n",
      "    (0): Sequence1dBlock(\n",
      "      (layer): LSTM(10, 256, batch_first=True)\n",
      "    )\n",
      "    (1): Sequence1dBlock(\n",
      "      (layer): LSTM(256, 256, batch_first=True)\n",
      "    )\n",
      "  )\n",
      "  (head): MultiLayerPerceptron(\n",
      "    (blocks): LayerList(\n",
      "      (0): LinearBlock(\n",
      "        (layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "        (activation): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = dl.models.RecurrentModel(\n",
    "    in_features=10, \n",
    "    hidden_features=[256, 256], \n",
    "    out_features=1,\n",
    "    return_cell_state=True,\n",
    "    rnn_type=\"LSTM\",\n",
    "    out_activation=torch.nn.Sigmoid,\n",
    "    bidirectional=False,\n",
    "    batch_first=True,\n",
    "    embedding=torch.nn.Embedding(\n",
    "        num_embeddings=10, \n",
    "        embedding_dim=10,\n",
    "    ),\n",
    ").create()\n",
    "\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer\n",
    "\n",
    "The `VisionTransformer` provides a base for vision transformers. Specific implementations will be provided in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT(\n",
      "  (patch_embedder): Patchify(\n",
      "    (layer): Conv2d(3, 128, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (positional_embedder): PositionalEmbedding(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoderLayer(\n",
      "    (blocks): LayerList(\n",
      "      (0-1): 2 x SequentialBlock(\n",
      "        (multihead): LayerDropoutSkipNormalization(\n",
      "          (normalization): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (layer): MultiheadSelfAttention(\n",
      "            (projection): Identity()\n",
      "            (attention): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "          (skip): Add()\n",
      "        )\n",
      "        (feed_forward): LayerDropoutSkipNormalization(\n",
      "          (normalization): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (layer): MultiLayerPerceptron(\n",
      "            (blocks): LayerList(\n",
      "              (0-1): 2 x LinearBlock(\n",
      "                (layer): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (activation): GELU(approximate='none')\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "          (skip): Add()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dense_top): MultiLayerPerceptron(\n",
      "    (blocks): LayerList(\n",
      "      (0): LinearBlock(\n",
      "        (layer): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (layer): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (layer): Linear(in_features=32, out_features=1, bias=True)\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformer = dl.models.ViT(\n",
    "    in_channels=3, \n",
    "    image_size=256, \n",
    "    patch_size=8, \n",
    "    hidden_features=[128, 128], \n",
    "    num_heads=8, \n",
    "    out_features=1,\n",
    ").create()\n",
    "\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Your Own Model from Deeplay Components\n",
    "\n",
    "Typically, you'll make your own model using the `Sequential` object.\n",
    "\n",
    "For example, you might combine both models and other components ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BackboneResnet18(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Layer[Conv2d](in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
      "        (normalization): Layer[BatchNorm2d](num_features=64)\n",
      "        (activation): Layer[ReLU](inplace=True)\n",
      "        (pool): Layer[MaxPool2d](kernel_size=3, stride=2, padding=1, ceil_mode=False, dilation=1)\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (blocks): Sequential(\n",
      "          (0-1): 2 x Conv2dBlock(\n",
      "            (shortcut_start): Conv2dBlock(\n",
      "              (layer): Layer[Identity](in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)\n",
      "            )\n",
      "            (blocks): Sequential(\n",
      "              (0): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=64)\n",
      "                (activation): Layer[ReLU]()\n",
      "              )\n",
      "              (1): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=64)\n",
      "              )\n",
      "            )\n",
      "            (shortcut_end): Add()\n",
      "            (activation): Layer[ReLU]()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (blocks): Sequential(\n",
      "          (0): Conv2dBlock(\n",
      "            (shortcut_start): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=64, out_channels=128, kernel_size=1, stride=2, padding=0)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "            )\n",
      "            (blocks): Sequential(\n",
      "              (0): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "                (activation): Layer[ReLU]()\n",
      "              )\n",
      "              (1): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "              )\n",
      "            )\n",
      "            (shortcut_end): Add()\n",
      "            (activation): Layer[ReLU]()\n",
      "          )\n",
      "          (1): Conv2dBlock(\n",
      "            (shortcut_start): Conv2dBlock(\n",
      "              (layer): Layer[Identity](in_channels=128, out_channels=128, kernel_size=1, stride=1, padding=0)\n",
      "            )\n",
      "            (blocks): Sequential(\n",
      "              (0): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "                (activation): Layer[ReLU]()\n",
      "              )\n",
      "              (1): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=128)\n",
      "              )\n",
      "            )\n",
      "            (shortcut_end): Add()\n",
      "            (activation): Layer[ReLU]()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv2dBlock(\n",
      "        (blocks): Sequential(\n",
      "          (0): Conv2dBlock(\n",
      "            (shortcut_start): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=128, out_channels=256, kernel_size=1, stride=2, padding=0)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "            )\n",
      "            (blocks): Sequential(\n",
      "              (0): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "                (activation): Layer[ReLU]()\n",
      "              )\n",
      "              (1): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "              )\n",
      "            )\n",
      "            (shortcut_end): Add()\n",
      "            (activation): Layer[ReLU]()\n",
      "          )\n",
      "          (1): Conv2dBlock(\n",
      "            (shortcut_start): Conv2dBlock(\n",
      "              (layer): Layer[Identity](in_channels=256, out_channels=256, kernel_size=1, stride=1, padding=0)\n",
      "            )\n",
      "            (blocks): Sequential(\n",
      "              (0): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "                (activation): Layer[ReLU]()\n",
      "              )\n",
      "              (1): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=256)\n",
      "              )\n",
      "            )\n",
      "            (shortcut_end): Add()\n",
      "            (activation): Layer[ReLU]()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): Conv2dBlock(\n",
      "        (blocks): Sequential(\n",
      "          (0): Conv2dBlock(\n",
      "            (shortcut_start): Conv2dBlock(\n",
      "              (layer): Layer[Conv2d](in_channels=256, out_channels=512, kernel_size=1, stride=2, padding=0)\n",
      "              (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "            )\n",
      "            (blocks): Sequential(\n",
      "              (0): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "                (activation): Layer[ReLU]()\n",
      "              )\n",
      "              (1): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "              )\n",
      "            )\n",
      "            (shortcut_end): Add()\n",
      "            (activation): Layer[ReLU]()\n",
      "          )\n",
      "          (1): Conv2dBlock(\n",
      "            (shortcut_start): Conv2dBlock(\n",
      "              (layer): Layer[Identity](in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0)\n",
      "            )\n",
      "            (blocks): Sequential(\n",
      "              (0): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "                (activation): Layer[ReLU]()\n",
      "              )\n",
      "              (1): Conv2dBlock(\n",
      "                (layer): Layer[Conv2d](in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
      "                (normalization): Layer[BatchNorm2d](num_features=512)\n",
      "              )\n",
      "            )\n",
      "            (shortcut_end): Add()\n",
      "            (activation): Layer[ReLU]()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (postprocess): Layer[Identity]()\n",
      "    (pool): Layer[AdaptiveAvgPool2d](output_size=(1, 1))\n",
      "  )\n",
      "  (1): SmallMLP(\n",
      "    (blocks): LayerList(\n",
      "      (0): LinearBlock(\n",
      "        (layer): Layer[Linear](in_features=512, out_features=32, bias=True)\n",
      "        (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "        (normalization): Layer[BatchNorm1d]()\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (layer): Layer[Linear](in_features=32, out_features=32, bias=True)\n",
      "        (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "        (normalization): Layer[BatchNorm1d]()\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (layer): Layer[Linear](in_features=32, out_features=1, bias=True)\n",
      "        (activation): Layer[Identity]()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = dl.Sequential(\n",
    "    dl.models.BackboneResnet18(in_channels=3, pool_output=True),\n",
    "    dl.models.SmallMLP(in_features=512, out_features=1),\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... you can also add PyTorch layers and customize the properties of the Deeplay components ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvolutionalEncoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Layer[Conv2d](in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
      "        (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (pool): Layer[MaxPool2d](kernel_size=2, stride=2)\n",
      "        (layer): Layer[Conv2d](in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
      "        (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (pool): Layer[MaxPool2d](kernel_size=2, stride=2)\n",
      "        (layer): Layer[Conv2d](in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
      "        (activation): Layer[LeakyReLU](negative_slope=0.2)\n",
      "      )\n",
      "      (3): Conv2dBlock(\n",
      "        (pool): Layer[MaxPool2d](kernel_size=2, stride=2)\n",
      "        (layer): Layer[Conv2d](in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
      "        (activation): Layer[Identity]()\n",
      "      )\n",
      "    )\n",
      "    (postprocess): Layer[Identity]()\n",
      "  )\n",
      "  (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (2): MultiLayerPerceptron(\n",
      "    (blocks): LayerList(\n",
      "      (0): LinearBlock(\n",
      "        (layer): Layer[Linear](in_features=128, out_features=1, bias=True)\n",
      "        (activation): Layer[Identity]()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = dl.Sequential(\n",
    "    dl.ConvolutionalEncoder2d(3, [16, 32, 64], 128),\n",
    "    torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    dl.MultiLayerPerceptron(128, [], 1),\n",
    ")\n",
    "model[..., \"activation\"].isinstance(torch.nn.ReLU) \\\n",
    "    .configure(torch.nn.LeakyReLU, negative_slope=0.2)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Model by Subclassing\n",
    "\n",
    "A model is just a `DeeplayModule` sublass like any other. For some applications, it might be more convenient to subclass the model and implement the `.forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (backbone): ConvolutionalEncoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (3): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "    (postprocess): Identity()\n",
      "  )\n",
      "  (regression_head): SmallMLP(\n",
      "    (blocks): LayerList(\n",
      "      (0): LinearBlock(\n",
      "        (layer): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.05)\n",
      "        (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (layer): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.05)\n",
      "        (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (layer): Linear(in_features=32, out_features=1, bias=True)\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classification_head): SmallMLP(\n",
      "    (blocks): LayerList(\n",
      "      (0): LinearBlock(\n",
      "        (layer): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.05)\n",
      "        (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (layer): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.05)\n",
      "        (normalization): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (layer): Linear(in_features=32, out_features=10, bias=True)\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyModel(dl.DeeplayModule):\n",
    "    \"\"\"Class for my model subclassing DeeplayModule.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize my module.\"\"\"\n",
    "        self.backbone = dl.ConvolutionalEncoder2d(\n",
    "            in_channels=3, \n",
    "            hidden_channels=[16, 32, 64], \n",
    "            out_channels=128,\n",
    "        )\n",
    "        self.regression_head = dl.models.SmallMLP(\n",
    "            in_features=128, \n",
    "            out_features=1,\n",
    "        )\n",
    "        self.classification_head = dl.models.SmallMLP(\n",
    "            in_features=128, \n",
    "            out_features=10,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate forward pass for my module.\"\"\"\n",
    "        x = self.backbone(x)\n",
    "        reg = self.regression_head(x)\n",
    "        cls = self.classification_head(x)\n",
    "        return reg, cls\n",
    "    \n",
    "model = MyModel().create()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Weights\n",
    "\n",
    "Deeplay provides some weight initialization methods, which you can use before the model is built ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean weights first layer: 1.0270298719406128\n",
      "STD weights first layer: 0.9969273209571838\n"
     ]
    }
   ],
   "source": [
    "backbone = dl.ConvolutionalEncoder2d(\n",
    "    in_channels=3, \n",
    "    hidden_channels=[16, 32, 64], \n",
    "    out_channels=128,\n",
    ")\n",
    "\n",
    "initializer = dl.initializers.Normal(mean=1, std=1.0)\n",
    "backbone.initialize(initializer)  # Before building the model.\n",
    "backbone.build()\n",
    "\n",
    "print(f\"Mean weights first layer: {backbone.blocks[0].layer.weight.mean()}\") \n",
    "print(f\"STD weights first layer: {backbone.blocks[0].layer.weight.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or after the model is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean weights first layer: 0.9408934712409973\n",
      "STD weights first layer: 1.0349736213684082\n"
     ]
    }
   ],
   "source": [
    "backbone = dl.ConvolutionalEncoder2d(\n",
    "    in_channels=3, \n",
    "    hidden_channels=[16, 32, 64], \n",
    "    out_channels=128,\n",
    ")\n",
    "\n",
    "initializer = dl.initializers.Normal(mean=1, std=1.0)\n",
    "backbone.build()\n",
    "backbone.initialize(initializer)  # After building the model.\n",
    "\n",
    "print(f\"Mean weights first layer: {backbone.blocks[0].layer.weight.mean()}\") \n",
    "print(f\"STD weights first layer: {backbone.blocks[0].layer.weight.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditionally Initializing the Weights\n",
    "\n",
    "You can also conditionally initialize the weights. For example, this can be useful if you want to use different initialization methods for different parts of the model.\n",
    "\n",
    "There are two ways to do this. The first is to set the `targets` parameter of the initializer, which should be a tuple of module types that should be initialized ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d \n",
      "Mean 0.014395988546311855 \n",
      "STD 1.0669336318969727 \n",
      "\n",
      "BatchNorm2d \n",
      "Mean 1.0 \n",
      "STD 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "backbone = dl.ConvolutionalEncoder2d(\n",
    "    in_channels=3, \n",
    "    hidden_channels=[16, 32, 64], \n",
    "    out_channels=128,\n",
    ")\n",
    "backbone.normalized()  # Add normalization to be initialized differently.\n",
    "\n",
    "norm_initializer = dl.initializers.Normal(\n",
    "    mean=0, \n",
    "    std=1.0, \n",
    "    targets=(torch.nn.Conv2d,),   # Target layers for normal initialization.\n",
    ")\n",
    "backbone.initialize(norm_initializer)\n",
    "\n",
    "const_initializer = dl.initializers.Constant(\n",
    "    weight=1, \n",
    "    bias=0, \n",
    "    targets=(torch.nn.BatchNorm2d,),  # Target layers for constant initialization.\n",
    ")\n",
    "backbone.initialize(const_initializer)\n",
    "\n",
    "backbone.build()\n",
    "\n",
    "print(\"Conv2d \\n\"\n",
    "      f\"Mean {backbone.blocks[0].layer.weight.mean()} \\n\" \n",
    "      f\"STD {backbone.blocks[0].layer.weight.std()} \\n\")\n",
    "print(\"BatchNorm2d \\n\" \n",
    "      f\"Mean {backbone.blocks[0].normalization.weight.mean()} \\n\" \n",
    "      f\"STD {backbone.blocks[0].normalization.weight.std()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the second way is to use the selector syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d \n",
      "Mean 0.030025742948055267 \n",
      "STD 0.9641354084014893 \n",
      "\n",
      "BatchNorm2d \n",
      "Mean 1.0 \n",
      "STD 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "backbone = dl.ConvolutionalEncoder2d(\n",
    "    in_channels=3, \n",
    "    hidden_channels=[16, 32, 64], \n",
    "    out_channels=128,\n",
    ")\n",
    "backbone.normalized()\n",
    "\n",
    "norm_initializer = dl.initializers.Normal(mean=0, std=1)\n",
    "const_initializer = dl.initializers.Constant(weight=1, bias=0)\n",
    "\n",
    "backbone[..., \"layer\"].all.initialize(norm_initializer)\n",
    "backbone[..., \"normalization\"].all.initialize(const_initializer)\n",
    "\n",
    "backbone.build()\n",
    "\n",
    "print(\"Conv2d \\n\"\n",
    "      f\"Mean {backbone.blocks[0].layer.weight.mean()} \\n\" \n",
    "      f\"STD {backbone.blocks[0].layer.weight.std()} \\n\")\n",
    "print(\"BatchNorm2d \\n\" \n",
    "      f\"Mean {backbone.blocks[0].normalization.weight.mean()} \\n\" \n",
    "      f\"STD {backbone.blocks[0].normalization.weight.std()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You can specify the tensors to be initialized during weight initialization. This is done with the `tensors` parameter. For example:\n",
    "\n",
    "```python\n",
    "initializer = Normal()\n",
    "model.initialize(initializer, tensors=(\"weight\", \"bias\"))  # Bias and weights (default).\n",
    "model.initialize(initializer, tensors=\"weight\")  # Only weight.\n",
    "model.initialize(initializer, tensors=\"bias\")  # Only bias.\n",
    "\n",
    "const = Constant()\n",
    "model.initialize(const, tensors=\"bias\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
