{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Core Objects in Deeplay\n",
    "\n",
    "Before starting to implement applications in Deeplay, let's define the most important objects for building a model in Deeplay:\n",
    "\n",
    "- **Application:** The highest-level object that defines the neural network architecture and training process. This includes the loss, the optimizer, and the training logic. Applications are typically task-oriented, such as `ImageClassifier` and `ObjectDetector`.\n",
    "\n",
    "- **Model:** A model is a specific neural network architecture that is typically part of an application. Examples include `ResNet` and `VGG`.\n",
    "\n",
    "- **Component:** A model is usually made by combining multiple components. Components are much more flexible than models. Examples include `MultiLayerPerceptron` and `ConvolutionalEncoder2d`.\n",
    "\n",
    "- **Block:** A block is a specific combination of layers that performs a small unit of calculation (for example layer plus activation). Blocks are the building blocks of components. They are the most flexible objects in Deeplay. Examples include `LinearBlock` and `Conv2dBlock`.\n",
    "\n",
    "- **Layer:** A layer consists of a single torch layer, such as `torch.nn.Linear` and `torch.nn.Conv2d`. Layers are the most basic building blocks in Deeplay.\n",
    "\n",
    "In the following sections, you'll create some examples of these obejcts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Deeplay\n",
    "\n",
    "Import `deeplay` (shortened to `dl`, as an abbreaviation of both *deeplay* and *deep learning*) ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the `torch.nn` namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "Starting with the most basic building block, a `Layer` is a single PyTorch layer. In this example, you'll create a linear layer with 10 input features and 5 output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer[Linear](in_features=10, out_features=5)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = dl.Layer(nn.Linear, in_features=10, out_features=5)\n",
    "\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify the layer after it's created using the `configure()` method to change any of its properties. For example, here you'll change the number of output features and remove the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer[Linear](in_features=10, out_features=3, bias=False)\n"
     ]
    }
   ],
   "source": [
    "linear_layer.configure(out_features=3, bias=False)\n",
    "\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the layer into a pure PyTorch module, you simply need to build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=3, bias=False)\n"
     ]
    }
   ],
   "source": [
    "torch_layer = linear_layer.build()\n",
    "\n",
    "print(torch_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Most Deeplay objects are modified in place when you build them. If you want to keep the original object, either call `.create()` or `.new().build()` instead; in this way, layers will return torch objects, while most other deeplay objects will return Deeplay objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks\n",
    "\n",
    "Going up one level in the hierarchy, a `Block` is a combination of layers that performs a relatively simple calculation. They are usually a sequence of `Layer` objects, but can include other blocks or sequences of blocks as well. In this example, you'll create a block that consists of a linear layer followed by a ReLU activation function.\n",
    "\n",
    "Start by creating a `LinearBlock`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearBlock(\n",
      "  (layer): Layer[Linear](in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_block = dl.LinearBlock(in_features=10, out_features=5)\n",
    "\n",
    "print(linear_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is no activation by default, there are a few ways to add an activation to a block. \n",
    "\n",
    "The most common way to add an activation is to use the `activation` argument when creating the block ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearBlock(\n",
      "  (layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_block_with_activation = dl.LinearBlock(\n",
    "    in_features=10, \n",
    "    out_features=5, \n",
    "    activation=dl.Layer(nn.ReLU),\n",
    ").build()\n",
    "\n",
    "print(linear_block_with_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... you can also add an activation to an existing block using the `.activated()` method ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearBlock(\n",
      "  (layer): Layer[Linear](in_features=10, out_features=5, bias=True)\n",
      "  (activation): Layer[ReLU]()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_block_activated = dl.LinearBlock(in_features=10, out_features=5)\n",
    "\n",
    "linear_block_activated.activated(nn.ReLU)\n",
    "\n",
    "print(linear_block_activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... you can use the `.configure()` method to add an activation to a block (this way is rarely needed) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearBlock(\n",
      "  (layer): Layer[Linear](in_features=10, out_features=5, bias=True)\n",
      "  (activation): Layer[ReLU]()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_block_configured = dl.LinearBlock(in_features=10, out_features=5)\n",
    "\n",
    "linear_block_configured.configure(activation=dl.Layer(nn.ReLU))\n",
    "\n",
    "print(linear_block_configured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or you can use the `.append()` method (also this way is rarely used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearBlock(\n",
      "  (layer): Layer[Linear](in_features=10, out_features=5, bias=True)\n",
      "  (activation): Layer[ReLU]()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_block_appended = dl.LinearBlock(in_features=10, out_features=5)\n",
    "\n",
    "linear_block_appended.append(dl.Layer(nn.ReLU), name=\"activation\")\n",
    "\n",
    "print(linear_block_appended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "A `Component` is a collection of blocks combined to form a more complex neural network component. \n",
    "\n",
    "In this example, you'll create a simple feedforward neural network component with two linear blocks, each followed by a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=10, out_features=32, bias=True)\n",
      "      (activation): Layer[ReLU]()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=5, bias=True)\n",
      "      (activation): Layer[ReLU]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp_component = dl.MultiLayerPerceptron(\n",
    "    in_features=10,\n",
    "    hidden_features=[32],\n",
    "    out_features=5,\n",
    "    out_activation=dl.Layer(nn.ReLU)\n",
    ")   \n",
    "\n",
    "print(mlp_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the component is made of blocks, you can access the blocks using the `.blocks` attribute. This allows you to modify the blocks after the component has been created. For example, you can change the activation function of the first block to a Sigmoid function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=10, out_features=32, bias=True)\n",
      "      (activation): Layer[Sigmoid]()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=5, bias=True)\n",
      "      (activation): Layer[ReLU]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp_component.blocks[0].activated(nn.Sigmoid)\n",
    "\n",
    "print(mlp_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or you can remove the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=10, out_features=32, bias=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=5, bias=True)\n",
      "      (activation): Layer[ReLU]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp_component.blocks[0].remove(\"activation\")\n",
    "\n",
    "print(mlp_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The next step up in the hierarchy is a `Model`, which is a specific neural network architecture. Examples are lecun-5 and resnet. For fully connected networks, there are few widely recognized standard models. \n",
    "\n",
    "Here, you'll simply instantiate a small MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=10, out_features=32, bias=True)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "      (normalization): Layer[BatchNorm1d]()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=32, bias=True)\n",
      "      (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "      (normalization): Layer[BatchNorm1d]()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=5, bias=True)\n",
      "      (activation): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "small_mlp = dl.models.SmallMLP(in_features=10, out_features=5)\n",
    "\n",
    "print(small_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Finally, an `Application` is the highest level of abstraction in Deeplay. It defines the neural network architecture and training process, including the loss, optimizer, and training logic. \n",
    "\n",
    "In this example, you'll create a simple classifier application that uses the previously defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (optimizer): Adam[Adam](lr=0.001)\n",
      "  (train_metrics): MetricCollection,\n",
      "    prefix=train\n",
      "  )\n",
      "  (val_metrics): MetricCollection,\n",
      "    prefix=val\n",
      "  )\n",
      "  (test_metrics): MetricCollection,\n",
      "    prefix=test\n",
      "  )\n",
      "  (model): SmallMLP(\n",
      "    (blocks): LayerList(\n",
      "      (0): LinearBlock(\n",
      "        (layer): Layer[Linear](in_features=10, out_features=32, bias=True)\n",
      "        (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "        (normalization): Layer[BatchNorm1d]()\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (layer): Layer[Linear](in_features=32, out_features=32, bias=True)\n",
      "        (activation): Layer[LeakyReLU](negative_slope=0.05)\n",
      "        (normalization): Layer[BatchNorm1d]()\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (layer): Layer[Linear](in_features=32, out_features=5, bias=True)\n",
      "        (activation): Layer[Identity]()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = dl.Classifier(small_mlp, optimizer=dl.Adam(lr=0.001))\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The Deeplay optimizer is a wrapper around the torch optimizer and is used to delay the attribution of parameters from the models until after the model is actually created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
