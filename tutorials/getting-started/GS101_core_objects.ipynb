{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Core Objects in Deeplay\n",
    "\n",
    "Before starting to implement applications in Deeplay, let's define the most important objects for building a model in Deeplay:\n",
    "- **Application:** The main object that defines the neural network architecture and training process. This includes the loss, the optimizer, and the training logic. They are typically task-oriented, such as `ImageClassifier`, `ObjectDetector`.\n",
    "- **Model:** A model is a specific neural network architecture that is part of an application. Examples include `ResNet`, `VGG`.\n",
    "- **Component:** A model is usually made by combining multiple components. Components are much more flexible than models. Examples include `MultiLayerPerceptron`, `ConvolutionalEncoder2d`.\n",
    "- **Block:** A block is a specific combination of layers that performs a small unit of calculation (for example `layer->activation`). Blocks are the building blocks of components, and are the most flexible objects in Deeplay. Examples include `LinearBlock`, `Conv2dBlock`.\n",
    "- **Layer:** A layer consists of a single torch layer, such as `torch.nn.Linear`, `torch.nn.Conv2d`. Layers are the most basic building blocks in Deeplay.\n",
    "\n",
    "In the following sections, you'll create some examples of these obejcts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Deeplay\n",
    "\n",
    "Import `deeplay` (shortened to `dl`, as an abbreaviation of both `deeplay` and `deeplearning`) ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the `torch.nn` namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Layers\n",
    "\n",
    "Starting with the most basic building block, a `Layer` is a single PyTorch layer. In this example, you'll create a linear layer with 10 input features and 5 output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer[Linear](in_features=10, out_features=5)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = dl.Layer(nn.Linear, in_features=10, out_features=5)\n",
    "\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify the layer after it's created using the `configure()` method to change any of its properties. For example, here you'll change the number of output features and remove the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer[Linear](in_features=10, out_features=3, bias=False)\n"
     ]
    }
   ],
   "source": [
    "linear_layer.configure(out_features=3, bias=False)\n",
    "\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the layer into a pure PyTorch module, you simply need to build it.\n",
    "\n",
    "**NOTE:** Most Deeplay objects are modified in place when you build them. If you want to keep the original object, either call `.create()` or `.new().build()` instead; in this way, layers will return torch objects, while most other deeplay objects will return Deeplay objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=3, bias=False)\n"
     ]
    }
   ],
   "source": [
    "torch_layer = linear_layer.build()\n",
    "\n",
    "print(torch_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Blocks\n",
    "\n",
    "Going up one level in the hierarchy, a `Block` is a combination of layers that performs a relatively simple calculation. They are usually a sequence of `Layer` objects, but can include other blocks or sequences of blocks as well. In this example, you'll create a block that consists of a linear layer followed by a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deeplay' has no attribute 'LinearBlock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m linear_block \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinearBlock\u001b[49m(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(linear_block)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deeplay' has no attribute 'LinearBlock'"
     ]
    }
   ],
   "source": [
    "linear_block = dl.LinearBlock(in_features=10, out_features=5)\n",
    "\n",
    "print(linear_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no activation by default. There are a few ways to add an activation to a block. The most common way is to use the `activation` argument when creating the block ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_block.activated(nn.ReLU)\n",
    "\n",
    "print(linear_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... you can also add an activation to an existing block using the `activated()` method ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_block_with_activation = dl.LinearBlock(in_features=10, out_features=5, activation=dl.Layer(nn.ReLU)).build()\n",
    "\n",
    "print(linear_block_with_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... finally you can use `.configure(activation=...)` to add an activation to a block (this final way is rarely needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Components\n",
    "\n",
    "A `Component` is a collection of blocks that are combined to form a more complex neural network component. In this example, you'll create a simple feedforward neural network component with two linear blocks, each followed by a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=10, out_features=32)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (normalization): Layer[Identity](num_features=32)\n",
      "      (dropout): Layer[Dropout](p=0)\n",
      "    )\n",
      "    (1): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=5)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (normalization): Layer[Identity](num_features=5)\n",
      "      (dropout): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp_component = dl.MultiLayerPerceptron(\n",
    "    in_features=10,\n",
    "    hidden_features=[32],\n",
    "    out_features=5,\n",
    "    out_activation=dl.Layer(nn.ReLU)\n",
    ")   \n",
    "\n",
    "print(mlp_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the component is made of blocks, you can access the blocks using the `blocks` attribute. This allows you to modify the blocks after the component has been created. For example, you can change the activation function of the first block to a Sigmoid function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LayerActivationNormalizationDropout' object has no attribute 'activated'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivated\u001b[49m(nn\u001b[38;5;241m.\u001b[39mSigmoid)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(mlp_component)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LayerActivationNormalizationDropout' object has no attribute 'activated'"
     ]
    }
   ],
   "source": [
    "mlp_component.blocks[0].activated(nn.Sigmoid)\n",
    "\n",
    "print(mlp_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or you can remove the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (blocks): LayerList(\n",
      "    (0): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=10, out_features=32)\n",
      "      (normalization): Layer[Identity](num_features=32)\n",
      "      (dropout): Layer[Dropout](p=0)\n",
      "    )\n",
      "    (1): LayerActivationNormalizationDropout(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=5)\n",
      "      (activation): Layer[ReLU]()\n",
      "      (normalization): Layer[Identity](num_features=5)\n",
      "      (dropout): Layer[Identity]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp_component.blocks[0].remove(\"activation\")\n",
    "\n",
    "print(mlp_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models\n",
    "\n",
    "The next step up in the hierarchy is a `Model`, which is a specific neural network architecture. Examples would be lecun-5, resnet. For fully connected networks, there are few widely recognized standard models. Here, you'll simply instantiate a small MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deeplay.models' has no attribute 'SmallMLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m small_mlp \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSmallMLP\u001b[49m(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(small_mlp)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deeplay.models' has no attribute 'SmallMLP'"
     ]
    }
   ],
   "source": [
    "small_mlp = dl.models.SmallMLP(in_features=10, out_features=5)\n",
    "\n",
    "print(small_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Applications\n",
    "\n",
    "Finally, an `Application` is the highest level of abstraction in Deeplay. It defines the neural network architecture and training process, including the loss, optimizer, and training logic. In this example, you'll create a simple classifier application that uses the previously defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_mlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m dl\u001b[38;5;241m.\u001b[39mClassifier(\u001b[43msmall_mlp\u001b[49m, optimizer\u001b[38;5;241m=\u001b[39mdl\u001b[38;5;241m.\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'small_mlp' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = dl.Classifier(small_mlp, optimizer=dl.Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deeplay optimizer is a wrapper around the torch optimizer which delays the attribution of parameters from the models until after the model is actually created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
