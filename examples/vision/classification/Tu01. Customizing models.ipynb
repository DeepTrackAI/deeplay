{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptorch as dtm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataloader = torch.utils.data.DataLoader(mnist, batch_size=32, num_workers=4)\n",
    "mnist_test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=32, num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating an ImageClassifier\n",
    "\n",
    "There are two ways of creating a DeepTorchModel. They operate almost identically, but there are cases where one may be more convenient than the other.\n",
    "\n",
    "These are:\n",
    "- Using the class constructor method (`ImageClassifier()`)\n",
    "- Using the `from_config` method (`ImageClassifier.from_config(config)`)\n",
    "\n",
    "The first option, `ImageClassifier()` can be convenient when no complex customization is needed, for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = dtm.ImageClassifier(num_classes=10)\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second option, `ImageClassifier.from_config(config)`, can be more convenient if the different components of `ImageClassifier` should be modified: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dtm.Config().num_classes(10)\n",
    "classifier = dtm.ImageClassifier.from_config(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used the `Config` object. `Config` is the heart and soul of `DeepTorch`, and is a very powerful way of defining rules for how a `DeepTorch` model should be created.\n",
    "\n",
    "First, let's briefly introduce the syntax of Config. (TODO: create a separate more in-depth tutorial for Config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Config objects\n",
    "\n",
    "`Config` objects are collections of `rules`, which link some name or selection (`num_classes`) to a value (`10`). `Config` objects are designed to be defined in a single line. So, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(\n",
       "#.num_classes = 10\n",
       "#.batch_size = 128\n",
       "#.learning_rate = 0.001\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = (\n",
    "    dtm.Config()\n",
    "       .num_classes(10)\n",
    "       .batch_size(128)\n",
    "       .learning_rate(0.001)\n",
    ")\n",
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a simple configuration object with `num_classes`, `batch_size` and `learning_rate` defined.\n",
    "These can then be accessed using the `get` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get(\"num_classes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the `get_parameters` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_classes': 10, 'batch_size': 128, 'learning_rate': 0.001}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DeepTorchModule`s will take parameters from the `Config` object as required. \n",
    "\n",
    "There's much more to discover about `Config`, but we'll introduce these topics as we progress through this tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Customizing ImageClassifier using Config attributes\n",
    "\n",
    "`ImageClassifier` accepts the following arguments:\n",
    "- `num_classes`: Integer, number of classes (10 in MNIST)\n",
    "- `backbone`:    Specifier for the backbone module\n",
    "- `head`:        Specifier for the classification head module\n",
    "- `connector`    Specifier for the module connecting `backbone` to `head`.\n",
    "\n",
    "Let's focus on `connector`. To customize the `connector` of `ImageClassifier`, we can pass a `Config` object with our specification of the backbone. \n",
    "\n",
    "As mentioned above, there are to ways to create a `DeepTorchModel`. For this first example, we'll demonstrate both. In the later examples, we'll stick to the `from_config` syntax for clarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using constructor and no config\n",
    "classifier = dtm.ImageClassifier(\n",
    "    num_classes=10,\n",
    "    connector=nn.Flatten(),       \n",
    ")\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=2, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using from_config\n",
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "    .num_classes(2)\n",
    "    .connector(nn.Flatten())   \n",
    ")\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we passed `nn.Flatten()` as the connector. It is also possible to pass `nn.Flatten` without instantiating it. DeepTorch will instatiate the class correctly where it is needed. \n",
    "\n",
    "It is generally recommended to _not_ instantiate the class. In many cases, the model will want to create many instances of the class for use in different places. If we instantiate the class _outside_ the model, then that same class will be used in multiple places. For some operations (e.g. `nn.Flatten`, `nn.ReLU`), this is safe. But for most (e.g. `nn.Linear`, `nn.Conv2d`), it is not. \n",
    "\n",
    "Arguments used to instantiate the class can be passed after the class in `Config`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .connector(nn.Flatten, start_dim=1, end_dim=-1)\n",
    ")\n",
    "\n",
    "classifier = dtm.ImageClassifier.from_config(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .connector(nn.Flatten)\n",
    "        .connector.start_dim(1)\n",
    "        .connector.end_dim(-1)\n",
    ")\n",
    "classifier = dtm.ImageClassifier.from_config(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second syntax demonstrates another property of `Config`. It is trivial to define nested structures by simply chaining `.` accessors. We will see why this is useful later in this notebook. \n",
    "\n",
    "As a final example, here is how one would using global average pooling instead of flatten to connect the convolutional backbone with the classification head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .connector(nn.AdaptiveAvgPool2d, output_size=(1, 1))\n",
    ")\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Customizing the backbone\n",
    "\n",
    "Ok, customizing the connector is simple; it's just a single layer after all. How about the backbone, which would typically be made of many layers? \n",
    "\n",
    "Well, there are three main ways:\n",
    "\n",
    "1. DeepTorch modules\n",
    "2. DeepTorch templates\n",
    "3. Torch modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 DeepTorch modules\n",
    "\n",
    "The simplest way to customizing the backbone (or any other multi-layer component) is to stay in the `DeepTorch` ecosystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = (\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone(dtm.ConvolutionalEncoder)\n",
    ")\n",
    "classifier = dtm.ImageClassifier.from_config(config)\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, arguments to instantiate the class can also the set in the config. \n",
    "\n",
    "`ConvolutionalEncoder` takes two arguments:\n",
    "- `blocks`: specifier for each step of the encoder.\n",
    "- `depth`: Integer, number of blocks in the encoder\n",
    "\n",
    "As such, we should be able easily change the size of the encoder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=2, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = (\n",
    "    dtm.Config()\n",
    "        .num_classes()\n",
    "        .backbone(dtm.ConvolutionalEncoder, depth=2)\n",
    ")\n",
    "classifier = dtm.ImageClassifier.from_config(config)\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But aren't we back to the same problem? How do we customize the `backbone.blocks`? Well, now we're at a small enough scale that we can use templates:\n",
    "\n",
    "### 2.1.2 DeepTorch templates\n",
    "\n",
    "Templates are a convenient way of defining small blocks of layers. The syntax is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_template = dtm.Layer(\"layer\") >> dtm.Layer(\"activation\") >> dtm.Layer(\"pool\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `Layer` is a sort of placeholder for a module that has is assigned a name (`\"layer\"`, `\"activation\"`, `\"pool\"`). Any name is valid. `>>` is a piping operation. It just means that first `dtm.Layer(\"layer\")` is evaluated, then `dtm.Layer(\"activation\")` and finally `dtm.Layer(\"pool\")`.\n",
    "We can assign modules to these names in the config. Let's demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = (\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone(dtm.ConvolutionalEncoder)\n",
    "        .backbone.depth(2)\n",
    "        .backbone.blocks(block_template)\n",
    "\n",
    "        # We can now refer to our names layer, activation, pool\n",
    "        .backbone.blocks.layer(nn.LazyConv2d, kernel_size=3, padding=1)\n",
    "        .backbone.blocks.activation(nn.LeakyReLU, negative_slope=0.2)\n",
    "        .backbone.blocks.pool(nn.MaxPool2d)\n",
    ")\n",
    "classifier = dtm.ImageClassifier.from_config(config)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_with_skip = dtm.Layer(\"socket\") >> dtm.Layer(\"layer\") >> dtm.Layer(\"activation\") >> dtm.Layer(\"pool\")\n",
    "\n",
    "branch = dtm.OutputOf(\"encoder.blocks[4]\")  >> dtm.Layer(\"layer\") >> dtm.Layer(\"activation\")\n",
    "\n",
    "branches = [dtm.OutputOf(f\"encoder.blocks[{4-i}]\") >> dtm.Layer(\"layer\") >> dtm.Layer(\"activation\")\n",
    "            for i in range(4)]\n",
    "\n",
    "config = (\n",
    "    dtm.Config()\n",
    "        # Adding\n",
    "        .decoder.blocks[0](block_with_skip)\n",
    "        .decoder.blocks[0].socket(dtm.Concatenate, dim=1)\n",
    "        .decoder.blocks[0].socket.inputs[0](None)\n",
    "        .decoder.blocks[0].socket.inputs[1](dtm.OutputOf(\"encoder.blocks[4]\"))\n",
    "        .decoder.blocks[0].socket.inputs[1].layer(nn.Conv2d, kernel_size=1, bias=False)\n",
    "        .decoder.blocks[0].socket.inputs[1].activation(nn.LeakyReLU, negative_slope=0.2)\n",
    "\n",
    "\n",
    "        # Custom skip\n",
    "        .decoder.blocks[0](block_with_skip)\n",
    "        .decoder.blocks[0].socket(\n",
    "            dtm.Skip,\n",
    "            func=lambda a, b: nn.cat((a, b), dim=1),\n",
    "            inputs=(None, \"encoder.blocks[4].layer\"),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Alternative syntax\n",
    "        .decoder.blocks[0](block_with_skip)\n",
    "        .decoder.blocks[0].skip(dtm.Skip)\n",
    "        .decoder.blocks[0].skip.func(lambda a, b: nn.cat((a, b), dim=1))\n",
    "        .decoder.blocks[0].skip.inputs((None, \"encoder.blocks[4].layer\"))\n",
    "        .decoder.blocks[0].skip.dim(1)\n",
    "\n",
    "        # Multiple\n",
    "        .decoder.blocks[:4](block_template)\n",
    "        .decoder.blocks[:4].socket(nn.Concatenate, dim=1)\n",
    "        .decoder.blocks[:4].populate(\"socket.inputs[1]\", branches)\n",
    "        .decoder.blocks.socket.inputs[1](dtm.OutputOf(\"encoder.blocks[4]\"))\n",
    "        .decoder.blocks.socket.inputs[1].layer(nn.Conv2d, kernel_size=1, bias=False)\n",
    "        .decoder.blocks.socket.inputs[1].activation(nn.LeakyReLU, negative_slope=0.2)\n",
    "\n",
    "        # Removing\n",
    "        .decoder.blocks[2].socket(dtm.NoSkip)\n",
    "\n",
    ")\n",
    "classifier = dtm.ImageClassifier.from_config(config)\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, a lot of what we specified are already the default values. We can omit everything that's already specified in the defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = (\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone.depth(2)\n",
    "        .backbone.blocks.activation(nn.LeakyReLU, negative_slope=0.2)\n",
    ")\n",
    "classifier = dtm.ImageClassifier.from_config(config)\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unsure about the structure of a submodule, you can use `_` and `__` as wildcard and double wildcard selectors. These operate exactly like `*` and `**` do in `glob`. In other words\n",
    "- `_` will match exactly one name\n",
    "- `__` will match any number of names, including zero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `_` is a wildcard that can match with any value\n",
    "It will match with `blocks`, since that is the only value in the list. \n",
    "As such, only the activation layers inside of backbone.blocks will change to `LeakyReLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone._.activation(nn.LeakyReLU, negative_slope=0.2)\n",
    ")\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two `_` wildcards. \n",
    "In this case, there will be two matches, and our rule will apply to both. The two matches are:\n",
    "1. `backbone.blocks.activation`\n",
    "2. `head.output.activation`\n",
    "\n",
    "Both will now be `LeakyReLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.2)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "    .num_classes(10)\n",
    "    ._._.activation(nn.LeakyReLU, negative_slope=0.2)\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__` will match with any nested structure, regardless of depth. For example, `Config().foo.__.baz` would apply to all of the following:\n",
    "- foo.baz\n",
    "- foo.bax.baz\n",
    "- foo.bax.bix.baz\n",
    "- foo.bax.bix.zig.baz\n",
    "etc.\n",
    "\n",
    "Here we see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.1)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.1)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.1)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): LeakyReLU(negative_slope=0.1)\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "    .num_classes(10)\n",
    "    .__.activation(nn.LeakyReLU)\n",
    "    .__.negative_slope(0.1)\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 - Toruch modules\n",
    "\n",
    "Of course, one can use standard torch modules. This can be useful if you want to use pretrained, standard networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\GU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): Identity()\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "backbone = torchvision.models.resnet18(pretrained=True)\n",
    "# We don't want the pooling or the fully connected layers, so we'll remove them:\n",
    "backbone.avgpool = nn.Identity()\n",
    "backbone.fc = nn.Identity()\n",
    "\n",
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "    .num_classes(10)\n",
    "    .backbone(backbone)\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Customizing backbone specific blocks\n",
    "\n",
    "So far, we have seen how to modify all blocks at once. However, one may want even more control over the final model by modifying individual blocks.\n",
    "\n",
    "We can achieve this using indexing with `blocks[index]`! Below we demonstrate how to set the template of individual blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (normalization): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (normalization): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first two blocks have normalization after the convolutional layer,\n",
    "block_0_and_1 = dtm.Layer(\"layer\") >> dtm.Layer(\"normalization\") >> dtm.Layer(\"activation\") >> dtm.Layer(\"pool\")\n",
    "\n",
    "# The third blocks has no normalization after the convolutional layer.\n",
    "block_2 = dtm.Layer(\"layer\") >> dtm.Layer(\"activation\") >> dtm.Layer(\"pool\")\n",
    "\n",
    "# The fourth blocks has no pooling and no normalization after the convolutional layer.\n",
    "block_3 = dtm.Layer(\"layer\") >> dtm.Layer(\"activation\")\n",
    "\n",
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone.blocks[:2](block_0_and_1) # will apply to the first two blocks\n",
    "        .backbone.blocks[2](block_2) # will apply to the third blocks\n",
    "        .backbone.blocks[3](block_3) # will apply to the fourth blocks\n",
    "        \n",
    "        # will apply to all blocks, but only block 0 and 1 use it.\n",
    "        .backbone.blocks.normalization(nn.LazyBatchNorm2d)\n",
    "        \n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, one can also set Layers to `Identity` where one does not want them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (normalization): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (normalization): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (normalization): Identity()\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (normalization): Identity()\n",
       "        (activation): ReLU()\n",
       "        (pool): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An alternative way to do create the same model as above:\n",
    "block = dtm.Layer(\"layer\") >> dtm.Layer(\"normalization\") >> dtm.Layer(\"activation\") >> dtm.Layer(\"pool\")\n",
    "classifier = dtm.ImageClassifier(\n",
    "    num_classes=10,\n",
    "    backbone=dtm.Config()\n",
    "                .blocks(block, normalization=nn.LazyBatchNorm2d)\n",
    "                .blocks[2:4].normalization(nn.Identity) # will apply to the third and fourth blocks\n",
    "                .blocks[3].pool(nn.Identity) # will apply to the fourth block\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some attributes, it's expected that each block has a different value for that attribute. A common example is `out_channels`, which is expected to increase deeper in the network. Following what we know so far, one could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone.blocks[0].layer.out_channels(4)\n",
    "        .backbone.blocks[1].layer.out_channels(8)\n",
    "        .backbone.blocks[2].layer.out_channels(16)\n",
    "        .backbone.blocks[3].layer.out_channels(32)\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this can be a bit verbose. In these cases, one can optionally use the `populate` method to set all values at once. `populate` can take values either from a list or a function. Let's demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone.blocks[0:4].populate(\"layer.out_channels\", [4, 8, 16, 32])\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (val_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone.blocks[0:4].populate(\"layer.out_channels\", lambda i: 4 * 2**i)\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.extra\n",
    "\n",
    "As a bonus, we can also start directly from templates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Template(\n",
       "  (backbone): ConvolutionalEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Template(\n",
       "        (layer): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Template(\n",
       "        (layer): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Template(\n",
       "        (layer): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Template(\n",
       "        (layer): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (connector): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): CategoricalClassificationHead(\n",
       "    (output): Template(\n",
       "      (layer): LazyLinear(in_features=0, out_features=2, bias=True)\n",
       "      (activation): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_template = dtm.Layer(\"backbone\") >> dtm.Layer(\"connector\") >> dtm.Layer(\"head\")\n",
    "\n",
    "classifier = classifier_template.from_config(\n",
    "    dtm.Config()\n",
    "        .backbone(dtm.ConvolutionalEncoder)\n",
    "        .connector(nn.Flatten)\n",
    "        .head(dtm.CategoricalClassificationHead)\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, keep in mind that the resulting model will be a pure `pytorch` model, and not a `pytorch_lightning` model. As such, many of the conveniences of `pytorch_lightning` are unavailable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                          | Params\n",
      "---------------------------------------------------------------\n",
      "0 | backbone     | ConvolutionalEncoder          | 24.4 K\n",
      "1 | connector    | Flatten                       | 0     \n",
      "2 | head         | CategoricalClassificationHead | 5.8 K \n",
      "3 | val_accuracy | Accuracy                      | 0     \n",
      "---------------------------------------------------------------\n",
      "30.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.2 K    Total params\n",
      "0.121     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96715769d1d74190be011977958ede3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46ffaa94abb4aeab6e4e890b40b0345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7604bd09fa426db0cdf089ab66789b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4b0a5e480a435599306af99dfa9b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba59d7ab4bd46d9ab8d71569af8755c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2887f71f3274f6b88a439674c5799ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62783a0c2ee42a89270395f8f58af72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The first two blocks have normalization after the convolutional layer,\n",
    "block_0_and_1 = dtm.Layer(\"layer\") >> dtm.Layer(\"normalization\") >> dtm.Layer(\"activation\") >> dtm.Layer(\"pool\")\n",
    "\n",
    "# The third blocks has no normalization after the convolutional layer.\n",
    "block_2 = dtm.Layer(\"layer\") >> dtm.Layer(\"activation\") >> dtm.Layer(\"pool\")\n",
    "\n",
    "# The fourth blocks has no pooling and no normalization after the convolutional layer.\n",
    "block_3 = dtm.Layer(\"layer\") >> dtm.Layer(\"activation\")\n",
    "\n",
    "dtm.Skip(\n",
    "    dtm.Config().encoder.blocks[2],\n",
    "    dtm.Config().decoder.blocks[4]\n",
    ")\n",
    "\n",
    "classifier = dtm.ImageClassifier.from_config(\n",
    "    dtm.Config()\n",
    "        .num_classes(10)\n",
    "        .backbone.blocks[:2](block_0_and_1) # will apply to the first two blocks\n",
    "        .backbone.blocks[2](block_2) # will apply to the third blocks\n",
    "        .backbone.blocks[3](block_3) # will apply to the fourth blocks\n",
    "        .decoder.blocks(dtm.Skip() >> dtm.Layer())\n",
    "        .decoder.blocks.skip(nn.Concatenate, aux=\"encoder.block\"\n",
    "        # will apply to all blocks, but only block 0 and 1 use it.\n",
    "        .backbone.blocks.normalization(nn.LazyBatchNorm2d)\n",
    "        \n",
    ")\n",
    "\n",
    "# Not necessary, but to get the number of parameters we first need to run the model with a batch of data\n",
    "# This is because the model is initialized lazily\n",
    "classifier(torch.rand(1, 1, 28, 28))\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=5, accelerator=\"cuda\")\n",
    "trainer.fit(classifier, mnist_dataloader, mnist_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:134: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at c:\\Users\\GU\\DeepTorch\\examples\\vision\\classification\\lightning_logs\\version_26\\checkpoints\\epoch=4-step=9375.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at c:\\Users\\GU\\DeepTorch\\examples\\vision\\classification\\lightning_logs\\version_26\\checkpoints\\epoch=4-step=9375.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2495e6fc6c4401b54d67e907b3e753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">        Test metric        </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.9857000112533569     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span><span style=\"color: #800080; text-decoration-color: #800080\">    1.4754879474639893     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.9857000112533569    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   1.4754879474639893    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.9857000112533569, 'test_loss': 1.4754879474639893}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(dataloaders=mnist_test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
